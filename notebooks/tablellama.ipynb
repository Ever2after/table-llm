{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/guest/anaconda3/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/guest/anaconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/guest/anaconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import argparse\n",
    "# import textwrap\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_seg}\\n\\n### Question:\\n{question}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(instruction, question, input_seg=None):\n",
    "  if input:\n",
    "    return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input_seg=input_seg, question=question)\n",
    "  else:\n",
    "    return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(\n",
    "    item, model, tokenizer, temperature=0.6, top_p=0.9, max_gen_len=4096, use_cache=True\n",
    "):\n",
    "    def response(item):\n",
    "    # def response(material, question, material_type=\"\", material_title=None):\n",
    "        # material = read_txt_file(material)\n",
    "        # prompt = format_prompt(material, question, material_type, material_title)\n",
    "        prompt = generate_prompt(instruction = item[\"instruction\"], input_seg = item[\"input_seg\"], question = item[\"question\"])\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "        out = tokenizer.decode(output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "\n",
    "        out = out.split(prompt)[1].strip()\n",
    "        return out\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, test_data):\n",
    "    # if args.flash_attn:\n",
    "    #     replace_llama_attn()\n",
    "\n",
    "    # Set RoPE scaling factor\n",
    "    config = transformers.AutoConfig.from_pretrained(\n",
    "        args.base_model,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "\n",
    "    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "    if orig_ctx_len and args.context_size > orig_ctx_len:\n",
    "        scaling_factor = float(math.ceil(args.context_size / orig_ctx_len))\n",
    "        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        args.base_model,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.resize_token_embeddings(32001)\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        args.base_model,\n",
    "        cache_dir=args.cache_dir,\n",
    "        model_max_length=args.context_size if args.context_size > orig_ctx_len else orig_ctx_len,\n",
    "        # padding_side=\"right\",\n",
    "        padding_side=\"left\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    # with open(args.input_data_file, \"r\") as f:\n",
    "    #     test_data = json.load(f)\n",
    "    \n",
    "    # import random\n",
    "    # test_data = random.sample(test_data, k=3)\n",
    "\n",
    "    test_data_pred = []\n",
    "    for i in tqdm(range(len(test_data))):\n",
    "        item = test_data[i]\n",
    "        new_item = {}\n",
    "        respond = build_generator(item, model, tokenizer, temperature=args.temperature, top_p=args.top_p,\n",
    "                              max_gen_len=args.max_gen_len, use_cache=not args.flash_attn)   # the temperature and top_p are highly different with previous alpaca exp, pay attention to this if there is sth wrong later\n",
    "        output = respond(item)\n",
    "\n",
    "        new_item[\"idx\"] = i\n",
    "        # new_item[\"table_id\"] = test_data[i][\"table_id\"]\n",
    "        new_item[\"instruction\"] = test_data[i][\"instruction\"]\n",
    "        new_item[\"input_seg\"] = test_data[i][\"input_seg\"]\n",
    "        new_item[\"question\"] = test_data[i][\"question\"]\n",
    "        # new_item[\"ground_truth\"] = test_data[i][\"ground_truth\"]\n",
    "        new_item[\"output\"] = test_data[i][\"output\"]\n",
    "        new_item[\"predict\"] = output\n",
    "\n",
    "        test_data_pred.append(new_item)\n",
    "        # import pdb\n",
    "        # pdb.set_trace() \n",
    "        \n",
    "    return test_data_pred\n",
    "        \n",
    "    # with open(args.output_data_file, \"w\") as f:\n",
    "    #     json.dump(test_data_pred, f, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='arg parser')\n",
    "    parser.add_argument('--base_model', type=str, default=\"osunlp/TableLlama\")\n",
    "    parser.add_argument('--cache_dir', type=str, default=\"./cache\")\n",
    "    parser.add_argument('--context_size', type=int, default=-1, help='context size during fine-tuning')\n",
    "    parser.add_argument('--flash_attn', type=bool, default=False, help='')\n",
    "    parser.add_argument('--temperature', type=float, default=0.6, help='')\n",
    "    parser.add_argument('--top_p', type=float, default=0.9, help='')\n",
    "    parser.add_argument('--max_gen_len', type=int, default=512, help='')\n",
    "    parser.add_argument('--input_data_file', type=str, default='input_data/', help='')\n",
    "    parser.add_argument('--output_data_file', type=str, default='output_data/', help='')\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [{\n",
    "    'instruction': 'This is a table QA task. The goal of this task is to answer the question given the table',\n",
    "    'input_seg': '[TAB] col: year|city|[SEP]|1896|athens|[SEP]|1900|paris|[SEP]|1904|st. louis|[SEP]|2004|athens|[SEP]|2008|beijing|[SEP]|2012|london]',\n",
    "    'question': 'In which year did beijing host the Olympic Games?',\n",
    "    'output': '2008'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc890ad13e74c22ab97449425cfa65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'idx': 0,\n",
       "  'instruction': 'This is a table QA task. The goal of this task is to answer the question given the table',\n",
       "  'input_seg': '[TAB] col: year|city|[SEP]|1896|athens|[SEP]|1900|paris|[SEP]|1904|st. louis|[SEP]|2004|athens|[SEP]|2008|beijing|[SEP]|2012|london]',\n",
       "  'question': 'In which year did beijing host the Olympic Games?',\n",
       "  'output': '2008',\n",
       "  'predict': '2008.0</s>'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parse_config()\n",
    "result = main(args, test_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
